---
title: "ML Basics"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
tutorial:
  id: "ml-basics"
  version: 0.5
---

## Setup

### 

Load needed packages.

```{r}
library(learnr)
library(mlbench)
library(boot)
```

### Data

In this notebook, we use the Boston Housing data set. "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms."

Source: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html

```{r}
data(BostonHousing2)
boston <- BostonHousing2
head(boston)
```

We use the `head` function to print the first few rows of the dataset.

## Regression in R

###

In this section, we begin with estimating a fairly simple regression model. We use the `glm` function in the stats package to fit a linear model using the median home value as the outcome and four variables as predictors. For linear regression we can also us the `lm` function, whereas if we want to fit generalized linear regression we need to use `glm`.

We can use the `summary` function to view the coefficient estimates, p-values, AIC, deviances to comment on the model fit. For this model we see that all the predictors are significant at 5% level of significance.

```{r}
m1 <- glm(medv ~ crim + chas + age + lstat, data = boston)
summary(m1)
```

###

Some more information about our first model. We use the `anova` function to get the Analysis of Variance (or deviance) table for fitted model objects m1. This is particulary useful in comparing nested models.

The plot function on the model object provided different plots to check model diagnostics like the residual vs fitted plot should show randomness instead of a specific pattern for a good fit. The Normal Q-Q plot compares quantiles of the data with those of Normal distribution and deviation from the egalitarian like (which is the case here) means the errors do not follow Normal distribution. We can also identify outliers, leverage and influential points from the residual vs. fitted or residual vs. leverage points. For example we see that 37 is an influential point having high residual value and Cook's distance.

```{r}
anova(m1)
plot(m1)
```

We can use `predict` to compute predicted home values based on our regression model. We compare the observed and fitted values for first 5 observations using the head function below.

```{r}
boston$pred1 <- predict(m1)
head(boston[,c(5,20)])
```

###

Next, we fit an extended model that includes `lstat` squared as an additional predictor variable. We use `I(lstat^2)` to include squared of the predictor (along with the predictor itself) in the model formula. The summary of the model object shows that both lstat and it's squared are significant at 5% level of significance.

```{r}
m2 <- glm(medv ~ crim + chas + age + lstat + I(lstat^2), data = boston)
summary(m2)
```

###

Both of the previous models were fitted using the full data set. Evaluating the prediction performance of these models on the same data gives us their training error. Here, we compute the training MSE. MSE is defined as the average of the squared difference between the observed and predicted values. So we compar the medv column from the dataset with the value from the `predict` function. We see that the MSE value is 36.25 for m1 and 26.10 for m2, as a result of lower MSE we prefer m2.

```{r}
mean((predict(m1) - boston$medv)^2)
mean((predict(m2) - boston$medv)^2)
```

## Train and test set

###

However, to get an estimate of the test error we have to proceed differently. A simple option is to split the data into a train and test set by random. Here we use `sample` function to prepare and 80 to 20 percent split of the BOston dataset. A seed is used to create reproducible results.

```{r}
set.seed(7345)
train <- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))
```

The resulting object gives us the row positions of the sampled elements. We use these positions to split the data into two pieces. So train dataset is obtained with the rownumbers in train and test is obtained with a `-` in front to get the remaining rows. Train dataset has 404 rows and test has 102 observations.

```{r}
boston_train <- BostonHousing2[train,]
boston_test <- BostonHousing2[-train,]
```

###

Now, refit the previous regression models using the training set only (which is `boston_train`). We store the model objects in m3 and m4.

```{r}
m3 <- glm(medv ~ crim + chas + age + lstat, data = boston_train)
m4 <- glm(medv ~ crim + chas + age + lstat + I(lstat^2), data = boston_train)
```

On this basis, we use these models to predict home values in the hold-out test set (which is `boston_test`).

```{r}
pred3 <- predict(m3, newdata = boston_test)
pred4 <- predict(m4, newdata = boston_test)
```

###

And evaluate the prediction performance in the test set using MSE as defined before. Here we see that the MSE values are 46.83 and 36.11 respecively.

```{r}
mean((pred3 - boston_test$medv)^2)
mean((pred4 - boston_test$medv)^2)
```

## Regression and CV

###

Another (better) evaluation approach is to use cross-validation. To demonstrate how cross-validation works, we will build our own CV loop by hand. We start by shuffling the data with `sample()` function. We then create 10 random `folds` (groups) and check the number of observations in each fold using `table` function. The folds have 50 or 51 observations.

```{r}
set.seed(7346)
boston <- boston[sample(nrow(boston)),]
folds <- cut(seq(1, nrow(boston)), breaks = 10, labels = FALSE)
table(folds)
```

###

In the following loop, each group is used as a hold-out fold once per iteration (`test_data`). The other groups (`train_data`) are used to fit the regression model, which is then evaluated on the hold-out fold. This results in fitting model 10 times and obtaining 10 test MSEs, one for each iteration. The `print` function inside the loop prints out the MSE values.

```{r}
pred <- rep(NA, nrow(boston))
for(i in 1:10){
    holdout <- which(folds==i)
    test_data <- boston[holdout, ]
    train_data <- boston[-holdout, ]
    
    m <- glm(medv ~ crim + chas + age + lstat, data = train_data)
    pred[holdout] <- predict(m, newdata = test_data)
    print(mean((pred[holdout] - boston$medv[holdout])^2))
}
```

###

Computing the MSE over all hold-out observations, using the mean function, gives us the cross-validated MSE, which is 37.4.

```{r}
mean((pred - boston$medv)^2)
```

###

Cross-validation is implemented in many R packages, which typically allow more flexibility. For regression, we could e.g. use `cv.glm()` from the `boot` package to calculate the estimated K-fold cross-validation prediction error for generalized linear models.. The default setting is to run leave-one-out cross-validation. For more information see `?cv.glm`. The `delta` function returns a vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.

```{r}
cv.err <- cv.glm(boston, m1)
cv.err$delta
```

###

We could also do 5-fold with specifying K=5 inside the `cv.glm` function as follows:

```{r}
cv.err5 <- cv.glm(boston, m1, K = 5)
cv.err5$delta
```

###

...or 10-fold CV.

```{r}
cv.err10 <- cv.glm(boston, m1, K = 10)
cv.err10$delta
```

###

On this basis, we can now check whether the extended model does not only yield a lower training error, but also performs better when using hold-out sets for model evaluation as the value now is 26.13.

```{r}
cv.err10.2 <- cv.glm(boston, m2, K = 10)
cv.err10.2$delta
```